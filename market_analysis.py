# market_analysis.py

import yfinance as yf
import pandas as pd
import streamlit as st
from datetime import date, timedelta, datetime
import requests
from fredapi import Fred
import os # file operation
from io import StringIO # ç”¨äºå°†ä¸‹è½½çš„æ–‡æœ¬å†…å®¹åŒ…è£…æˆæ–‡ä»¶å¯¹è±¡

# ----------------------------------------------------
# Function to get S&P 500 Symbols from Wikipedia
# ----------------------------------------------------
@st.cache_data(ttl=timedelta(days=30)) # Cache symbols for 30 day
def get_sp500_symbols():
    OUTPUT_FILENAME = 'sp500_symbols.csv'
    temp_filename = "temp_download.csv" # ä¸´æ—¶æ–‡ä»¶ç”¨äºä¿å­˜ä¸‹è½½å†…å®¹
    """
    Fetches the latest S&P 500 component list from Wikipedia.
    """
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    
    # CRITICAL FIX: Add User-Agent to bypass 403 Forbidden error
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # "å°è¯•ä» Wikipedia è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨..."
        # Use requests to download content with headers
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() 

        # Use pandas to read the HTML content (response.text)
        tables = pd.read_html(response.text)
        
        sp500_table = None
        for table in tables:
            if 'Symbol' in table.columns and 'Security' in table.columns:
                sp500_table = table
                break
        
        if sp500_table is None:
            st.error("æ— æ³•åœ¨ Wikipedia é¡µé¢æ‰¾åˆ° S&P 500 æˆåˆ†è‚¡è¡¨æ ¼ã€‚")
            return []

        symbols = sp500_table['Symbol'].tolist()

        # æ ¸å¿ƒæ›¿æ¢é€»è¾‘ï¼šå°†æ‰€æœ‰ç‚¹å· '.' æ›¿æ¢ä¸ºè¿å­—ç¬¦ '-'
        cleaned_symbols = [symbol.replace('.', '-') for symbol in symbols]
        #st.success(f"æˆåŠŸè·å–å¹¶æ¸…ç† {len(cleaned_symbols)} ä¸ª S&P 500 æˆåˆ†è‚¡ä»£ç ã€‚")
        df_symbols = pd.DataFrame(cleaned_symbols, columns=['Symbol'])
        #st.info(df_symbols)
        # ä¿å­˜åˆ° CSV æ–‡ä»¶
        df_symbols.to_csv(OUTPUT_FILENAME, index=False)
        
        return cleaned_symbols
        
    except requests.exceptions.HTTPError as e:
        st.error(f"è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ (HTTP é”™è¯¯: {e})ã€‚è¯·æ£€æŸ¥ User-Agent æˆ–ç›®æ ‡ URLã€‚")
        return []
    except requests.exceptions.RequestException as e:
        st.error(f"è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ (ç½‘ç»œæˆ–è¶…æ—¶é”™è¯¯): {e}")
        return []
    except Exception as e:
        st.error(f"è§£æ S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥: {e}")
        return []


# ----------------------------------------------------
# Function to download stock data
# ----------------------------------------------------
#@st.cache_data(ttl=timedelta(days=1)) # Cache stock data everyday
def get_sp500_stock_data():
    """Downloads historical price data for all S&P 500 symbols."""
    FILE_PATH = 'spy500_data.csv'
    TTL_SECONDS = 60*60*4 # 240 minute Time-To-Live (TTL)
    sp500_symbols = get_sp500_symbols() 
    
    if not sp500_symbols:
        st.warning("æœªèƒ½è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨ï¼Œæ— æ³•ä¸‹è½½è‚¡ç¥¨æ•°æ®ã€‚")
        return None
        
    # --- 1. Check if cached CSV exists and is fresh ---
    if os.path.exists(FILE_PATH):
        file_mod_time = os.path.getmtime(FILE_PATH)
        age_seconds = datetime.now().timestamp() - file_mod_time
        
        if age_seconds < TTL_SECONDS:
            # st.info(f"ğŸ’¾ ä»æœ¬åœ°æ–‡ä»¶åŠ è½½è‚¡ç¥¨æ•°æ® ({FILE_PATH})...")
            try:
                # Load data from CSV, handling the MultiIndex header structure
                data = pd.read_csv(
                    FILE_PATH, 
                    header=[0, 1], 
                    index_col=0, 
                    parse_dates=True
                )
                st.success("æ•°æ®åŠ è½½æˆåŠŸã€‚")
                return data
            except Exception as e:
                # If loading fails, log error and proceed to download
                st.error(f"åŠ è½½æœ¬åœ°æ–‡ä»¶å¤±è´¥: {e}")
        else:
            st.info(f"ğŸ“… æœ¬åœ°æ•°æ®å·²è¿‡æœŸï¼Œå°†é‡æ–°ä¸‹è½½")
            
            end_date = date.today()
            start_date = end_date - timedelta(days=700)  # Set start date for required history (9000 days provides a long history)
        
            st.write(f"ğŸ“ˆ æ­£åœ¨ä¸‹è½½ {len(sp500_symbols)} æ”¯ S&P 500 æˆåˆ†è‚¡å†å²ä»·æ ¼æ•°æ®... (åˆæ¬¡è¿è¡Œè¾ƒæ…¢)")
        
            data = None
            try:
                # ä½¿ç”¨ concurrent downloads (threads) æ¥å¤„ç†å¤§ç¬¦å·åˆ—è¡¨
                data = yf.download(tickers=sp500_symbols, start=start_date, end=end_date, group_by='ticker', progress=False, auto_adjust=True, repair=True)
                
                # Filter out tickers that failed to download or are entirely empty
                valid_tickers = [ticker for ticker in sp500_symbols if (ticker, 'Close') in data.columns]
                
                if len(valid_tickers) < len(sp500_symbols):
                    st.warning(f"æ³¨æ„: {len(sp500_symbols) - len(valid_tickers)} æ”¯è‚¡ç¥¨æ•°æ®æœªèƒ½å®Œå…¨ä¸‹è½½ã€‚")
        
                # --- 3. Save to CSV before returning ---
                if data is not None:
                    # Save data to CSV, maintaining the MultiIndex structure
                    data.to_csv(FILE_PATH, index=True)
                    st.success(f"âœ… æ•°æ®ä¸‹è½½å®Œæˆå¹¶å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶: {FILE_PATH}")       
                return data
        
            except Exception as e:
                st.error(f"ä¸‹è½½S&P 500æ•°æ®å¤±è´¥: {e}")
                return None

# ----------------------------------------------------
# Function to calculate market breadth
# ----------------------------------------------------
def calculate_market_breadth_history(stock_data: pd.DataFrame):
    """
    è®¡ç®—å†å²ä¸Šæ¯å¤©æœ‰å¤šå°‘æˆåˆ†è‚¡çš„è‚¡ä»·ä½äº20æ—¥å’Œ60æ—¥å‡çº¿ä¸Šæ–¹ã€‚
    
    Returns:
        pd.DataFrame: ç´¢å¼•ä¸ºæ—¥æœŸï¼Œåˆ—ä¸º '20DMA_Breadth' å’Œ '60DMA_Breadth' ç™¾åˆ†æ¯”ã€‚
    """
    
    sp500_symbols = get_sp500_symbols()
    
    # è·å–æ‰€æœ‰æ”¶ç›˜ä»·æ•°æ®åˆ—
    close_data = stock_data.xs('Close', level=1, axis=1)

    if close_data.empty or not sp500_symbols:
        return pd.DataFrame()

    # ç¡®ä¿åªä¿ç•™S&P 500æˆåˆ†è‚¡çš„åˆ—
    close_data = close_data[sp500_symbols]
    
    # 1. è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„å†å²ç§»åŠ¨å¹³å‡çº¿
    ma_20 = close_data.rolling(window=20).mean()
    ma_60 = close_data.rolling(window=60).mean()

    # 2. æ¯”è¾ƒï¼šæ”¶ç›˜ä»·æ˜¯å¦é«˜äºç§»åŠ¨å¹³å‡çº¿ (å¾—åˆ° True/False DataFrame)
    # True è¢«è§†ä¸º 1, False è¢«è§†ä¸º 0
    above_20ma_df = (close_data > ma_20).astype(int)
    above_60ma_df = (close_data > ma_60).astype(int)

    # 3. æ±‡æ€»ï¼šè®¡ç®—æ¯å¤©æœ‰å¤šå°‘è‚¡ç¥¨é«˜äºMA
    # (å³æŒ‰è¡Œæ±‚å’Œ)
    daily_20ma_count = above_20ma_df.sum(axis=1)
    daily_60ma_count = above_60ma_df.sum(axis=1)

    # 4. è®¡ç®—æ¯å¤©ç¬¦åˆMAè®¡ç®—æ¡ä»¶çš„è‚¡ç¥¨æ€»æ•°
    # å¦‚æœæ”¶ç›˜ä»·æˆ–MAæ˜¯NaNï¼Œåˆ™è¯¥è‚¡ç¥¨ä¸åˆæ ¼ (å³ rolling window ä¸è¶³)
    daily_eligible_count = (
        close_data.notna() & ma_60.notna()
    ).sum(axis=1)
    
    # 5. è®¡ç®—ç™¾åˆ†æ¯”
    # é¿å…é™¤ä»¥é›¶
    breadth_history = pd.DataFrame({
        '20DMA_Breadth': (daily_20ma_count / daily_eligible_count) * 100,
        '60DMA_Breadth': (daily_60ma_count / daily_eligible_count) * 100,
        'Eligible_Count': daily_eligible_count
    }).dropna()
    
    return breadth_history

# ----------------------------------------------------------------------
# IMPORTANT: New function for getting the LATEST SNAPSHOT (for sidebar)
# ----------------------------------------------------------------------

def get_latest_breadth_snapshot(breadth_history: pd.DataFrame):
    """
    ä»å†å²æ•°æ®ä¸­æå–æœ€æ–°çš„å¸‚åœºå®½åº¦å¿«ç…§ï¼Œç”¨äºä¾§è¾¹æ æ˜¾ç¤ºã€‚
    """
    if breadth_history.empty:
        return {
            "eligible_total": "N/A",
            "20DMA_count": "N/A", "20DMA_percentage": 0,
            "60DMA_count": "N/A", "60DMA_percentage": 0,
        }
    
    latest = breadth_history.iloc[-1]
    total = latest['Eligible_Count']
    
    # è®¡ç®—æœ€æ–°çš„è®¡æ•° (éœ€è¦å›åˆ°åŸå§‹é€»è¾‘ï¼Œæˆ–è€…å°†è®¡æ•°å­˜å‚¨åœ¨å†å²DFä¸­)
    # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾æˆ‘ä»¬åªå±•ç¤ºç™¾åˆ†æ¯”ã€‚
    # å¦‚æœè¦å±•ç¤ºè®¡æ•°ï¼Œæœ€å¥½åœ¨å†å²DFä¸­å­˜å‚¨è®¡æ•°ï¼Œæˆ–è€…å›åˆ°åŸå§‹è®¡ç®—æ–¹å¼è·å–æœ€æ–°å¿«ç…§ã€‚
    # é‰´äºæˆ‘ä»¬å·²é‡å†™å†å²DFï¼Œæˆ‘ä»¬åªä½¿ç”¨ç™¾åˆ†æ¯”å’Œæ€»æ•°ã€‚
    
    # æ³¨æ„ï¼šä¸ºäº†è®© rd_data.py çš„ä¾§è¾¹æ èƒ½å¤Ÿç»§ç»­å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åŒ…è£…æ•°æ®ç»“æ„ã€‚
    latest_snapshot = {
        "eligible_total": int(total),
        "20DMA_percentage": latest['20DMA_Breadth'],
        "60DMA_percentage": latest['60DMA_Breadth'],
        # ç”±äº historical calculation è¿‡ç¨‹å¤æ‚åŒ–äº† count æå–ï¼Œ
        # æš‚æ—¶ä½¿ç”¨ç™¾åˆ†æ¯”å’Œæ€»æ•°æ¥æ¨ç®— countã€‚
        "20DMA_count": int(latest['20DMA_Breadth'] / 100 * total), 
        "60DMA_count": int(latest['60DMA_Breadth'] / 100 * total),
    }
    return latest_snapshot

# --- å…³é”®ï¼šä» Streamlit Secrets è·å– API Key ---
try:
    FRED_API_KEY = st.secrets["FRED_API_KEY"]
except KeyError:
    FRED_API_KEY = None
    st.error("æ— æ³•æ‰¾åˆ° FRED_API_KEYã€‚è¯·å°†å…¶æ·»åŠ ä¸º Streamlit Secret ä»¥è·å–å¤±ä¸šç‡æ•°æ®ã€‚")


@st.cache_data(ttl=timedelta(days=1))
def get_unemployment_data():
    """
    ä½¿ç”¨ fredapi è·å–ç¾å›½å¤±ä¸šç‡ (UNRATE) æ•°æ®ã€‚
    """
    if not FRED_API_KEY:
        return pd.DataFrame()

    try:
        fred = Fred(api_key=FRED_API_KEY)
        
        # è·å– UNRATE ç³»åˆ— (æœˆåº¦æ•°æ®)
        unrate_series = fred.get_series('UNRATE') 
        
        if unrate_series is None or unrate_series.empty:
            st.warning("FRED API è¿”å›çš„å¤±ä¸šç‡æ•°æ®ä¸ºç©ºã€‚")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸º DataFrameï¼Œå¹¶é‡å‘½ååˆ—
        df_unrate = unrate_series.to_frame(name='Unemployment_Rate')
        
        # ç¡®ä¿ç´¢å¼•æ˜¯æ—¥æœŸæ—¶é—´æ ¼å¼ (é€šå¸¸æ˜¯è‡ªåŠ¨çš„)
        df_unrate.index.name = 'Date' 
        
        # ä¸ºäº†ä¸æ‚¨çš„å…¶ä»–æ—¥çº¿æ•°æ®åŒ¹é…ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œé™é‡‡æ ·æˆ–å¤„ç†ï¼Œä½†æœˆåº¦æ•°æ®å¯ä»¥ç›´æ¥ç»˜åˆ¶
        return df_unrate
        
    except Exception as e:
        st.error(f"è·å– FRED å¤±ä¸šç‡æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame()

#@st.cache_data(ttl=timedelta(days=1))
def get_highyield_data():
    """
    ä½¿ç”¨ fredapi è·å–æŒ‡å®š FRED ç³»åˆ—çš„æ•°æ®ã€‚
    """
    # 1. æ£€æŸ¥ API Key
    if not FRED_API_KEY:
        st.error("FRED API Key æœªè®¾ç½®ã€‚è¯·åœ¨ Streamlit Secrets æˆ–ä»£ç ä¸­è®¾ç½® FRED_API_KEYã€‚")
        return pd.DataFrame()

    try:
        # 2. åˆå§‹åŒ– Fred å®¢æˆ·ç«¯
        fred = Fred(api_key=FRED_API_KEY)
        
        # 3. è·å–æŒ‡å®šç³»åˆ—æ•°æ®ï¼Œå¹¶é™åˆ¶èµ·å§‹æ—¥æœŸ
        data_series = fred.get_series('BAMLH0A0HYM2')
        
        # 4. æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        if data_series is None or data_series.empty:
            st.warning(f"FRED API è¿”å›çš„ç³»åˆ— '{series_id}' æ•°æ®ä¸ºç©ºæˆ–è·å–å¤±è´¥ã€‚")
            return pd.DataFrame()
        
        # 5. è½¬æ¢ä¸º DataFrameï¼Œå¹¶é‡å‘½ååˆ—
        df_data = data_series.to_frame(name='Value')
        
        # 6. è®¾ç½®ç´¢å¼•åç§°
        df_data.index.name = 'Date'
        
        return df_data
        
    except Exception as e:
        st.error(f"è·å– FRED æ•°æ®å¤±è´¥ (Series ID: {series_id}): {e}")
        return pd.DataFrame()
