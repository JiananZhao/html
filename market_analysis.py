# market_analysis.py

import yfinance as yf
import pandas as pd
import streamlit as st
from datetime import date, timedelta
import requests

# ----------------------------------------------------
# Function to get S&P 500 Symbols from Wikipedia
# ----------------------------------------------------
@st.cache_data(ttl=timedelta(days=1)) # Cache symbols for 1 day
def get_sp500_symbols():
    """
    Fetches the latest S&P 500 component list from Wikipedia.
    """
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    
    # CRITICAL FIX: Add User-Agent to bypass 403 Forbidden error
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        st.info("å°è¯•ä» Wikipedia è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨...")
        
        # Use requests to download content with headers
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status() 

        # Use pandas to read the HTML content (response.text)
        tables = pd.read_html(response.text)
        
        sp500_table = None
        for table in tables:
            if 'Symbol' in table.columns and 'Security' in table.columns:
                sp500_table = table
                break
        
        if sp500_table is None:
            st.error("æ— æ³•åœ¨ Wikipedia é¡µé¢æ‰¾åˆ° S&P 500 æˆåˆ†è‚¡è¡¨æ ¼ã€‚")
            return []

        symbols = sp500_table['Symbol'].tolist()
        
        st.success(f"æˆåŠŸè·å– {len(symbols)} ä¸ª S&P 500 æˆåˆ†è‚¡ä»£ç ã€‚")
        return symbols

    except requests.exceptions.HTTPError as e:
        st.error(f"è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ (HTTP é”™è¯¯: {e})ã€‚è¯·æ£€æŸ¥ User-Agent æˆ–ç›®æ ‡ URLã€‚")
        return []
    except requests.exceptions.RequestException as e:
        st.error(f"è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥ (ç½‘ç»œæˆ–è¶…æ—¶é”™è¯¯): {e}")
        return []
    except Exception as e:
        st.error(f"è§£æ S&P 500 æˆåˆ†è‚¡åˆ—è¡¨å¤±è´¥: {e}")
        return []


# ----------------------------------------------------
# Function to download stock data
# ----------------------------------------------------
@st.cache_data(ttl=timedelta(hours=6)) # Cache stock data for 6 hours
def get_sp500_stock_data():
    """Downloads historical price data for all S&P 500 symbols."""
    
    sp500_symbols = get_sp500_symbols() 
    
    if not sp500_symbols:
        st.warning("æœªèƒ½è·å– S&P 500 æˆåˆ†è‚¡åˆ—è¡¨ï¼Œæ— æ³•ä¸‹è½½è‚¡ç¥¨æ•°æ®ã€‚")
        return None

    end_date = date.today()
    start_date = end_date - timedelta(days=90) # Need 90 days for 20 DMA calculation buffer

    st.write(f"ğŸ“ˆ æ­£åœ¨ä¸‹è½½ {len(sp500_symbols)} æ”¯ S&P 500 æˆåˆ†è‚¡å†å²ä»·æ ¼æ•°æ®... (åˆæ¬¡è¿è¡Œè¾ƒæ…¢)")
    
    try:
        # ä½¿ç”¨ concurrent downloads (threads) æ¥å¤„ç†å¤§ç¬¦å·åˆ—è¡¨
        data = yf.download(
            tickers=sp500_symbols,
            start=start_date,
            end=end_date,
            group_by='ticker',
            progress=False, 
            auto_adjust=True, 
            repair=True,
            # --- å…³é”®ä¿®å¤ï¼šç§»é™¤ 'max_workers' å’Œ 'threads' å‚æ•° ---
            # yfinance é»˜è®¤ä¼šè¿›è¡Œçº¿ç¨‹ä¸‹è½½ï¼Œä¸éœ€è¦é¢å¤–è®¾ç½®è¿™äº›å‚æ•°
        )
        
        # Filter out tickers that failed to download or are entirely empty
        valid_tickers = [ticker for ticker in sp500_symbols if (ticker, 'Close') in data.columns]
        
        if len(valid_tickers) < len(sp500_symbols):
            st.warning(f"æ³¨æ„: {len(sp500_symbols) - len(valid_tickers)} æ”¯è‚¡ç¥¨æ•°æ®æœªèƒ½å®Œå…¨ä¸‹è½½ã€‚")
            
        return data

    except Exception as e:
        st.error(f"ä¸‹è½½S&P 500æ•°æ®å¤±è´¥: {e}")
        return None


# ----------------------------------------------------
# Function to calculate market breadth
# ----------------------------------------------------
def calculate_market_breadth_history(stock_data: pd.DataFrame):
    """
    è®¡ç®—å†å²ä¸Šæ¯å¤©æœ‰å¤šå°‘æˆåˆ†è‚¡çš„è‚¡ä»·ä½äº20æ—¥å’Œ60æ—¥å‡çº¿ä¸Šæ–¹ã€‚
    
    Returns:
        pd.DataFrame: ç´¢å¼•ä¸ºæ—¥æœŸï¼Œåˆ—ä¸º '20DMA_Breadth' å’Œ '60DMA_Breadth' ç™¾åˆ†æ¯”ã€‚
    """
    
    sp500_symbols = get_sp500_symbols()
    
    # è·å–æ‰€æœ‰æ”¶ç›˜ä»·æ•°æ®åˆ—
    close_data = stock_data.xs('Close', level=1, axis=1)

    if close_data.empty or not sp500_symbols:
        return pd.DataFrame()

    # ç¡®ä¿åªä¿ç•™S&P 500æˆåˆ†è‚¡çš„åˆ—
    close_data = close_data[sp500_symbols]
    
    # 1. è®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„å†å²ç§»åŠ¨å¹³å‡çº¿
    ma_20 = close_data.rolling(window=20).mean()
    ma_60 = close_data.rolling(window=60).mean()

    # 2. æ¯”è¾ƒï¼šæ”¶ç›˜ä»·æ˜¯å¦é«˜äºç§»åŠ¨å¹³å‡çº¿ (å¾—åˆ° True/False DataFrame)
    # True è¢«è§†ä¸º 1, False è¢«è§†ä¸º 0
    above_20ma_df = (close_data > ma_20).astype(int)
    above_60ma_df = (close_data > ma_60).astype(int)

    # 3. æ±‡æ€»ï¼šè®¡ç®—æ¯å¤©æœ‰å¤šå°‘è‚¡ç¥¨é«˜äºMA
    # (å³æŒ‰è¡Œæ±‚å’Œ)
    daily_20ma_count = above_20ma_df.sum(axis=1)
    daily_60ma_count = above_60ma_df.sum(axis=1)

    # 4. è®¡ç®—æ¯å¤©ç¬¦åˆMAè®¡ç®—æ¡ä»¶çš„è‚¡ç¥¨æ€»æ•°
    # å¦‚æœæ”¶ç›˜ä»·æˆ–MAæ˜¯NaNï¼Œåˆ™è¯¥è‚¡ç¥¨ä¸åˆæ ¼ (å³ rolling window ä¸è¶³)
    daily_eligible_count = (
        close_data.notna() & ma_60.notna()
    ).sum(axis=1)
    
    # 5. è®¡ç®—ç™¾åˆ†æ¯”
    # é¿å…é™¤ä»¥é›¶
    breadth_history = pd.DataFrame({
        '20DMA_Breadth': (daily_20ma_count / daily_eligible_count) * 100,
        '60DMA_Breadth': (daily_60ma_count / daily_eligible_count) * 100,
        'Eligible_Count': daily_eligible_count
    }).dropna()
    
    return breadth_history

# ----------------------------------------------------------------------
# IMPORTANT: New function for getting the LATEST SNAPSHOT (for sidebar)
# ----------------------------------------------------------------------

def get_latest_breadth_snapshot(breadth_history: pd.DataFrame):
    """
    ä»å†å²æ•°æ®ä¸­æå–æœ€æ–°çš„å¸‚åœºå®½åº¦å¿«ç…§ï¼Œç”¨äºä¾§è¾¹æ æ˜¾ç¤ºã€‚
    """
    if breadth_history.empty:
        return {
            "eligible_total": "N/A",
            "20DMA_count": "N/A", "20DMA_percentage": 0,
            "60DMA_count": "N/A", "60DMA_percentage": 0,
        }
    
    latest = breadth_history.iloc[-1]
    total = latest['Eligible_Count']
    
    # è®¡ç®—æœ€æ–°çš„è®¡æ•° (éœ€è¦å›åˆ°åŸå§‹é€»è¾‘ï¼Œæˆ–è€…å°†è®¡æ•°å­˜å‚¨åœ¨å†å²DFä¸­)
    # ä¸ºç®€å•èµ·è§ï¼Œè¿™é‡Œå‡è®¾æˆ‘ä»¬åªå±•ç¤ºç™¾åˆ†æ¯”ã€‚
    # å¦‚æœè¦å±•ç¤ºè®¡æ•°ï¼Œæœ€å¥½åœ¨å†å²DFä¸­å­˜å‚¨è®¡æ•°ï¼Œæˆ–è€…å›åˆ°åŸå§‹è®¡ç®—æ–¹å¼è·å–æœ€æ–°å¿«ç…§ã€‚
    # é‰´äºæˆ‘ä»¬å·²é‡å†™å†å²DFï¼Œæˆ‘ä»¬åªä½¿ç”¨ç™¾åˆ†æ¯”å’Œæ€»æ•°ã€‚
    
    # æ³¨æ„ï¼šä¸ºäº†è®© rd_data.py çš„ä¾§è¾¹æ èƒ½å¤Ÿç»§ç»­å·¥ä½œï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åŒ…è£…æ•°æ®ç»“æ„ã€‚
    latest_snapshot = {
        "eligible_total": int(total),
        "20DMA_percentage": latest['20DMA_Breadth'],
        "60DMA_percentage": latest['60DMA_Breadth'],
        # ç”±äº historical calculation è¿‡ç¨‹å¤æ‚åŒ–äº† count æå–ï¼Œ
        # æš‚æ—¶ä½¿ç”¨ç™¾åˆ†æ¯”å’Œæ€»æ•°æ¥æ¨ç®— countã€‚
        "20DMA_count": int(latest['20DMA_Breadth'] / 100 * total), 
        "60DMA_count": int(latest['60DMA_Breadth'] / 100 * total),
    }
    return latest_snapshot
